{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:08:51.610819Z",
     "start_time": "2020-02-07T04:08:51.605832Z"
    }
   },
   "source": [
    "# 단어 빈도수 기반 자연어 처리\n",
    "> 단어의 빈도수\n",
    "- 의미가 있는 단어임을 알 수 있음\n",
    "- 문장 구분, 문맥을 파악할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:09:10.227548Z",
     "start_time": "2020-02-07T04:09:04.484166Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from tensorflow.keras.datasets import imdb\n",
    "#  한국어 형태소 분석기\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma \n",
    "from konlpy.tag import Twitter\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from gensim import corpora, models\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "from wordcloud import ImageColorGenerator\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:45:12.133322Z",
     "start_time": "2020-02-07T04:45:12.130330Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ['you know I want our love',\n",
    "         'I like you',\n",
    "         'what should I do']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 전체 단어의 수와 각 단어의 빈도수 조사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:23:30.799368Z",
     "start_time": "2020-02-07T04:23:30.795361Z"
    }
   },
   "source": [
    "## 1.1 단어 빈도수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:45:12.669034Z",
     "start_time": "2020-02-07T04:45:12.665103Z"
    }
   },
   "outputs": [],
   "source": [
    "srt = ''.join(corpus)\n",
    "words =srt.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:45:12.838013Z",
     "start_time": "2020-02-07T04:45:12.834074Z"
    }
   },
   "outputs": [],
   "source": [
    "freq = {}\n",
    "for w in words:\n",
    "    freq[w] = freq.get(w, 0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:45:12.991803Z",
     "start_time": "2020-02-07T04:45:12.986816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 1,\n",
       " 'know': 1,\n",
       " 'I': 2,\n",
       " 'want': 1,\n",
       " 'our': 1,\n",
       " 'loveI': 1,\n",
       " 'like': 1,\n",
       " 'youwhat': 1,\n",
       " 'should': 1,\n",
       " 'do': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:25:46.391670Z",
     "start_time": "2020-02-07T04:25:46.388677Z"
    }
   },
   "source": [
    "## 1.2 CountVectorizer: 단어 사전 만들기 + vectorize\n",
    "- 단어의 개수와 상관없이 고정된 길이의 벡터를 만들어서 \n",
    "- 단어가 있는지 없는지 보고, 있으면 1, 없으면 0으로 만들기 \n",
    "- 문장을 이루는 count를 계산해서 onehot encoding으로 바꾸기\n",
    "- 단어의 순서는 상관없음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:45:13.296271Z",
     "start_time": "2020-02-07T04:45:13.290337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 1]\n",
      " [0 0 1 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 1 0 1 0]]\n",
      "  (0, 8)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer()\n",
    "tf = vector.fit_transform(corpus)\n",
    "\n",
    "# dense한 행렬\n",
    "print(tf.toarray())\n",
    "\n",
    "# sparse한 행렬(희소 행렬)-> 메모리를 적게 사용\n",
    "print(tf)  # (행, 열)에서 1인 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:31:24.775205Z",
     "start_time": "2020-02-07T04:31:24.770218Z"
    }
   },
   "source": [
    "> 불용어 처리(stop word) \n",
    "- 글자수 1개인 것은 다 빼기 때문에 문장 길이와 1인 성분의 개수가 안맞음\n",
    "\n",
    "> 벡터라이즈\n",
    "- 유사도를 구하기 위해 거리를 잴 수 있음.\n",
    "- 분포도 파악할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:45:13.590340Z",
     "start_time": "2020-02-07T04:45:13.585352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 9)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray().shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:45:13.726850Z",
     "start_time": "2020-02-07T04:45:13.721863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 8,\n",
       " 'know': 1,\n",
       " 'want': 6,\n",
       " 'our': 4,\n",
       " 'love': 3,\n",
       " 'like': 2,\n",
       " 'what': 7,\n",
       " 'should': 5,\n",
       " 'do': 0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전:-> 인덱스갑 의미(빈도수 아님)\n",
    "vector.vocabulary_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:45:13.853971Z",
     "start_time": "2020-02-07T04:45:13.849975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vector.vocabulary_['you'])\n",
    "print(vector.vocabulary_.get('you')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:45:14.093482Z",
     "start_time": "2020-02-07T04:45:14.088558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do\n",
      "know\n",
      "like\n",
      "love\n",
      "our\n",
      "should\n",
      "want\n",
      "what\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "words = vector.get_feature_names()\n",
    "for word in words: \n",
    "    print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T04:45:14.366624Z",
     "start_time": "2020-02-07T04:45:14.361578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you 8\n",
      "know 1\n",
      "want 6\n",
      "our 4\n",
      "love 3\n",
      "like 2\n",
      "what 7\n",
      "should 5\n",
      "do 0\n"
     ]
    }
   ],
   "source": [
    "for key in vector.vocabulary_: \n",
    "    print(key, vector.vocabulary_[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. TF-IDF(Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "### TF, DF, TF-IDF 개념\n",
    "> TF : 내 문서만 보는 관점\n",
    "- 현재 문서(문장)에서 나타난 빈도수 (위의 벡터는 전체 문장에서)\n",
    "- tf가 높으면 그 문장에서 중요한 단어임.\n",
    "- 내 문서에서만 많이 나타나고, 다른 문장에서는 안나타날 수도 있음.\n",
    "- tf만 보기에는 문맥에 따라 단어의 중요도가 다를 수 있음.\n",
    "- <font color = 'red'>한문장에서 빈도수 / 문장 전체 단어 수</font>\n",
    "\n",
    "> DF(Documents Frequency): 다른 문서도 같이 보는 관점\n",
    "- 단어가 나타난 문서의 수\n",
    "- df가 높으면 흔한 단어이다. (the, a,...)\n",
    "- 얼마나 이 단어가 전체 문서에서 자주 반복되느냐..\n",
    "- 높을수록 중요하지 않음.\n",
    "- 낮으면 특수 문서에서만 나타난다는 뜻\n",
    "- IDF = 1/DF\n",
    "\n",
    "> TF_IDF\n",
    "- 특정 단어의 상대적인 빈도를 나타내는 값\n",
    "- 값이 클수록 (DF가 작음)내 문서에만 많이 언급되는 단어 -> 다른 문서에서는 언급 안됨=> 고유성. 대표성이 큼\n",
    "- 값이 작을수록(DF가 큼)  다른 문서에 잘 언급하는 단어 -> 현재 문서와 관련 없음.\n",
    "- 값이 클수록 내 문장을 대표하는(특징적인) 단어라는 뜻. \n",
    "- 단어의 순서는 고려하지 않음.\n",
    "- tf_IDF값이 높으려면 tf는 높고, idf는 높아야 함.-> 나만 많이 사용\n",
    "- tf-idf값이 낮으려면 tf는 낮고, idf도 낮아야 함.-> 안쓰는 단어\n",
    "\n",
    "### tf-idf 계산\n",
    "- 문서 수만큼 존재함-> 행렬값으로 나타남.\n",
    "- 단어로만 부과되는 값이 아니라, 내 문장내에서 부여됨.\n",
    "- tf-idf = tf * log(n/df)\n",
    "\n",
    "> df를 단순 수치로 비교할 경우\n",
    "- 전체 100건 중 사과라는 단어가  90이 나왔으면 df = 90 -> 확률은 0.09\n",
    "- 전체 10건중 사과가 5건 나왔으면 df = 5 -> 확률은 0.5\n",
    "- 단순 수치로 비교하면 안되기 때문에 df값을 0~1로 나누기 위해서\n",
    "- df를 n으로 나눠줌\n",
    "- idf는 역수이므로 n/df\n",
    "- df값은 문장이 많을 수록 기하급수적으로 커지기 때문에\n",
    "- 전체 문장 수로 나눠주면 최대값이 1이 됨.\n",
    "\n",
    "\n",
    "> log로 나누는 이유\n",
    "- 전체 문서가 1000건중 2개 나왔으면 idf = 500\n",
    "- 전체 문서  10만건 중 2개 나왔으면 idf = 5만\n",
    "- tf값은 그리 클 수가 없는 값인데 idf가 너무 크기 때문에 스케일을 맞추기 위해서 log값을 취함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T07:12:03.689248Z",
     "start_time": "2020-02-07T07:12:03.685258Z"
    }
   },
   "outputs": [],
   "source": [
    "sent = [\"오늘 휴일\", \n",
    "        \"휴일 오늘\", \n",
    "        \"휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.\", \n",
    "        \"폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니 다.\", \n",
    "        \" 내일 은 반가운 비 소식 이 있습니다.\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:34:35.852665Z",
     "start_time": "2020-02-07T05:34:35.847661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '휴일휴일', '오늘휴일', '인', '오늘', '도', '서쪽', '을', '중심', '으로', '폭염', '이', '이어졌는데요,', '내일', '은', '반가운', '비', '소식', '이', '있습니다.폭염', '을', '피해서', '휴일', '에', '놀러왔다가', '갑작스런', '비', '로', '인해', '망연자실', '하고', '있습니', '다.', '내일', '은', '반가운', '비', '소식', '이', '있습니다.']\n"
     ]
    }
   ],
   "source": [
    "words= ''.join(sent)\n",
    "words =words.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:29:34.198600Z",
     "start_time": "2020-02-07T05:29:34.192458Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sent) #문장 벡터화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:29:36.101255Z",
     "start_time": "2020-02-07T05:29:36.096259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 18)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.toarray().shape)\n",
    "\n",
    "# 5 : 문서의 수 -> 문서마다 결정\n",
    "# 18: 단어 수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:29:37.124497Z",
     "start_time": "2020-02-07T05:29:37.120481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17)\t0.6437444595062429\n",
      "  (0, 7)\t0.7652405313723362\n",
      "  (1, 17)\t0.6437444595062429\n",
      "  (1, 7)\t0.7652405313723362\n",
      "  (2, 12)\t0.28487998702172107\n",
      "  (2, 6)\t0.28487998702172107\n",
      "  (2, 4)\t0.28487998702172107\n",
      "  (2, 1)\t0.28487998702172107\n",
      "  (2, 9)\t0.35310140100264525\n",
      "  (2, 14)\t0.28487998702172107\n",
      "  (2, 8)\t0.35310140100264525\n",
      "  (2, 13)\t0.35310140100264525\n",
      "  (2, 5)\t0.35310140100264525\n",
      "  (2, 17)\t0.19893117008503197\n",
      "  (2, 7)\t0.23647612349029334\n",
      "  (3, 11)\t0.3542556015420614\n",
      "  (3, 16)\t0.3542556015420614\n",
      "  (3, 3)\t0.3542556015420614\n",
      "  (3, 10)\t0.3542556015420614\n",
      "  (3, 0)\t0.3542556015420614\n",
      "  (3, 2)\t0.3542556015420614\n",
      "  (3, 15)\t0.3542556015420614\n",
      "  (3, 14)\t0.28581118874948447\n",
      "  (3, 17)\t0.1995814265359179\n",
      "  (4, 12)\t0.5\n",
      "  (4, 6)\t0.5\n",
      "  (4, 4)\t0.5\n",
      "  (4, 1)\t0.5\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:29:41.403595Z",
     "start_time": "2020-02-07T05:29:41.400530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tfidf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:37:30.502109Z",
     "start_time": "2020-02-07T05:37:30.496125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.76524053, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.64374446],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.76524053, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.64374446],\n",
       "       [0.        , 0.28487999, 0.        , 0.        , 0.28487999,\n",
       "        0.3531014 , 0.28487999, 0.23647612, 0.3531014 , 0.3531014 ,\n",
       "        0.        , 0.        , 0.28487999, 0.3531014 , 0.28487999,\n",
       "        0.        , 0.        , 0.19893117],\n",
       "       [0.3542556 , 0.        , 0.3542556 , 0.3542556 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.3542556 , 0.3542556 , 0.        , 0.        , 0.28581119,\n",
       "        0.3542556 , 0.3542556 , 0.19958143],\n",
       "       [0.        , 0.5       , 0.        , 0.        , 0.5       ,\n",
       "        0.        , 0.5       , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.5       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat =tfidf_matrix.toarray()\n",
    "tfidf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:35:03.924515Z",
     "start_time": "2020-02-07T05:35:03.920529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 ['갑작스런', '내일', '놀러왔다가', '망연자실', '반가운', '서쪽', '소식', '오늘', '으로', '이어졌는데요', '인해', '있습니', '있습니다', '중심', '폭염', '피해서', '하고', '휴일']\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 확인\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "print(len(features), features)\n",
    "\n",
    "\n",
    "# 한 단어는 빼고,  겹치는 단어는 모두 제거되었음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:35:10.916392Z",
     "start_time": "2020-02-07T05:35:10.912402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'을', '오늘휴일', '휴일휴일', '인', '에', '로', '은', '도', '다.', '있습니다.', '이', '비', '이어졌는데요,', '있습니다.폭염'}\n"
     ]
    }
   ],
   "source": [
    "print(set(words)- set(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 결과 해석\n",
    "- tf-idf의 값이 0이라는 것은 그 문장에 안나왔다는 뜻 (tf = 0일테니까)\n",
    "- tf-idf의 값이 높다는 것은 내 문장에는 많이 나오고,다른 문장에서는 안나와야 함.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 특정 단어의 tf-idf값 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:45:41.225146Z",
     "start_time": "2020-02-07T05:45:41.220135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전에서의 인덱스 출력\n",
    "tfidf_vectorizer.vocabulary_.get('오늘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:47:04.829671Z",
     "start_time": "2020-02-07T05:47:04.824459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 17]\n"
     ]
    }
   ],
   "source": [
    "mat = np.asarray(tfidf_matrix.toarray())\n",
    "srch = ['오늘', '휴일']\n",
    "print([tfidf_vectorizer.vocabulary_.get(i) for i in srch ])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:50:27.380719Z",
     "start_time": "2020-02-07T05:50:27.375721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76524053, 0.64374446],\n",
       "       [0.76524053, 0.64374446],\n",
       "       [0.23647612, 0.19893117],\n",
       "       [0.        , 0.19958143],\n",
       "       [0.        , 0.        ]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srch_dtm = mat[:, [tfidf_vectorizer.vocabulary_.get(i) for i in srch]] # mat[:, [7,17]]\n",
    "srch_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> 해석\n",
    "- 값이 0인 것은 아예 그 문장에서 그 단어가 안나타났다는 뜻 (tf = 0이므로)\n",
    "- 값이 0이 아니라는 것은 그 문장에 나타났다는 뜻\n",
    "- 각 문장에 1번씩 나왔는데,왜 tf-idf값이 다른가? -> 값이 낮은 것은 tf값은 같지만, idf가 낮지 때문 -> 다른 문장에서 많이 사용되었다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장과 문장의 유사도 비교1\n",
    "- 문장끼리 유사도를 비교할 수도 있고,\n",
    "- 분류도 가능(비슷한 문장끼지 classification을 해서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:50:30.012445Z",
     "start_time": "2020-02-07T05:50:30.007475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40898499, 1.40898499, 0.43540729, 0.19958143, 0.        ])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(srch_dtm, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 문장별로 더했을 때 값이 클수록\n",
    "- '오늘', '휴일'이 각 문장들과 관련성이 높다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:50:46.940361Z",
     "start_time": "2020-02-07T05:50:46.936376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.40898499 1.40898499 0.43540729 0.19958143 0.        ]\n"
     ]
    }
   ],
   "source": [
    "score = srch_dtm.sum(axis = 1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:51:17.188677Z",
     "start_time": "2020-02-07T05:51:17.183729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 휴일/score:1.408984990878579\n",
      "휴일 오늘/score:1.408984990878579\n",
      "휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다./score:0.4354072935753253\n",
      "폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니 다./score:0.1995814265359179\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(score)):\n",
    "    if score[i] >0:\n",
    "        print('{}/score:{}'.format(sent[i], score[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:56:49.206631Z",
     "start_time": "2020-02-07T05:56:49.203639Z"
    }
   },
   "source": [
    "#### 문장과 문장의 유사도 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:00:00.982597Z",
     "start_time": "2020-02-07T06:00:00.978611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3]\n",
      "[[0.        0.       ]\n",
      " [0.        0.       ]\n",
      " [0.        0.       ]\n",
      " [0.3542556 0.3542556]\n",
      " [0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "mat = np.asarray(tfidf_matrix.toarray())\n",
    "srch=['갑작스런', '망연자실']\n",
    "print([  tfidf_vectorizer.vocabulary_.get(i) for i in srch])\n",
    "\n",
    "srch_dtm = mat[:, [  tfidf_vectorizer.vocabulary_.get(i) for i in srch]]\n",
    "\n",
    "#srch_dtm = mat[:, [ 7,17]]   \n",
    "#srch_dtm = mat[:, 7]   \n",
    "\n",
    "print(srch_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T05:58:47.324524Z",
     "start_time": "2020-02-07T05:58:47.319537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.28487999 0.3542556  0.5       ]\n",
      "휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다. / score : 0.28487998702172107\n",
      "폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니 다. / score : 0.3542556015420614\n",
      " 내일 은 반가운 비 소식 이 있습니다. / score : 0.5\n"
     ]
    }
   ],
   "source": [
    "score = srch_dtm.sum(axis=1)\n",
    "print(score)\n",
    "\n",
    "for i in range(len(score)):\n",
    "    if score[i] > 0:\n",
    "        print('{} / score : {}'.format(sent[i], score[i]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:09:49.693178Z",
     "start_time": "2020-02-07T06:09:49.687195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 17]\n",
      "[[0.         0.64374446]\n",
      " [0.         0.64374446]\n",
      " [0.         0.19893117]\n",
      " [0.3542556  0.19958143]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "mat = np.asarray(tfidf_matrix.toarray())\n",
    "srch=['갑작스런', '휴일']\n",
    "print([  tfidf_vectorizer.vocabulary_.get(i) for i in srch])\n",
    "\n",
    "srch_dtm = mat[:, [  tfidf_vectorizer.vocabulary_.get(i) for i in srch]]\n",
    "\n",
    "#srch_dtm = mat[:, [ 7,17]]   \n",
    "#srch_dtm = mat[:, 7]   \n",
    "\n",
    "print(srch_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:09:50.092938Z",
     "start_time": "2020-02-07T06:09:50.087008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64374446 0.64374446 0.19893117 0.55383703 0.        ]\n",
      "오늘 휴일 / score : 0.6437444595062429\n",
      "휴일 오늘 / score : 0.6437444595062429\n",
      "휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다. / score : 0.19893117008503197\n",
      "폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니 다. / score : 0.5538370280779793\n"
     ]
    }
   ],
   "source": [
    "score = srch_dtm.sum(axis=1)\n",
    "print(score)\n",
    "\n",
    "for i in range(len(score)):\n",
    "    if score[i] > 0:\n",
    "        print('{} / score : {}'.format(sent[i], score[i]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유사도 모델 만들기: SGDClassifier()이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:25:33.980379Z",
     "start_time": "2020-02-07T06:25:33.974406Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = ['This is the first document.',\n",
    "              'This is the second document.',\n",
    "              'And the third one.',\n",
    "              'Is this the first document?']\n",
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(sentences)\n",
    "y = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:26:53.037882Z",
     "start_time": "2020-02-07T06:26:53.030901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='perceptron',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델\n",
    "model =  SGDClassifier(loss='perceptron')  # 단층 퍼셉트론 모델 이용하기\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:27:05.418072Z",
     "start_time": "2020-02-07T06:27:05.414077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "X_pred = vect.transform(['My new document third'])\n",
    "y_pred = model.predict(X_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:28:19.387504Z",
     "start_time": "2020-02-07T06:28:19.382518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:26:10.213636Z",
     "start_time": "2020-02-07T06:26:10.210576Z"
    }
   },
   "source": [
    "> test과정에서\n",
    "- train에서 만들어진 단어 사전에 있는 단어만 고려하여 파악하기\n",
    "- 실제로 document, third로만 tfidf값을 계산해서 유사도 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:27:49.438590Z",
     "start_time": "2020-02-07T06:27:49.434639Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = ['This is the first document.',\n",
    "              'This is the second document.',\n",
    "              'And the third one.',\n",
    "              'Is this the first document?']\n",
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(sentences)\n",
    "y = [1,1,2,2]  # y라벨을 classification할수 있게 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:27:50.572242Z",
     "start_time": "2020-02-07T06:27:50.566290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='perceptron',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델\n",
    "model =  SGDClassifier(loss='perceptron')  # 단층 퍼셉트론 모델 이용하기\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T06:27:52.066890Z",
     "start_time": "2020-02-07T06:27:52.062877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "X_pred = vect.transform(['My new document third'])\n",
    "y_pred = model.predict(X_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
